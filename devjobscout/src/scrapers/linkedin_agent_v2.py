"""
Scraper mejorado de LinkedIn usando browser-use - Compatible con nueva arquitectura
"""
from browser_use import Agent, Browser, ChatBrowserUse, Tools
import asyncio
import json
from typing import List, Dict, Optional


class LinkedInScraper:
    """Scraper inteligente para LinkedIn Jobs"""

    def __init__(self):
        self.tools = Tools()
        self._setup_tools()
        self.scraped_jobs = []

    def _setup_tools(self):
        """Configura herramientas personalizadas"""

        @self.tools.action('Guarda las ofertas de LinkedIn en formato JSON')
        def guardar_ofertas(jobs_json: str) -> str:
            """
            Guarda ofertas extra√≠das

            Args:
                jobs_json: JSON string con la lista de ofertas
            """
            try:
                jobs = json.loads(jobs_json) if isinstance(jobs_json, str) else jobs_json
                self.scraped_jobs = jobs
                with open('src/data/linkedin_jobs.json', 'w', encoding='utf-8') as f:
                    json.dump(jobs, f, indent=2, ensure_ascii=False)
                return f"‚úÖ Guardadas {len(jobs)} ofertas de LinkedIn"
            except Exception as e:
                return f"‚ùå Error guardando ofertas: {e}"

    async def scrape(
        self,
        search_query: str,
        location: str = "Spain",
        max_results: int = 10,
        remote_only: bool = True,
        published_within_days: int = 7
    ) -> List[Dict]:
        """
        Scraper principal de LinkedIn

        Args:
            search_query: B√∫squeda (ej: "Python developer")
            location: Ubicaci√≥n
            max_results: N√∫mero m√°ximo de resultados
            remote_only: Solo trabajos remotos
            published_within_days: Ofertas publicadas en los √∫ltimos N d√≠as

        Returns:
            Lista de ofertas
        """
        llm = ChatBrowserUse()
        browser = Browser()

        # Construir filtros din√°micamente
        filters = []
        if published_within_days <= 1:
            filters.append("publicado √∫ltimas 24 horas")
        elif published_within_days <= 7:
            filters.append("publicado √∫ltima semana")
        else:
            filters.append("publicado √∫ltimo mes")

        if remote_only:
            filters.append("trabajo remoto")

        filters_text = f"Aplica filtros: {', '.join(filters)}"

        task = f"""
        1. Ve a LinkedIn Jobs: https://www.linkedin.com/jobs
        2. Busca: "{search_query}" en ubicaci√≥n "{location}"
        3. {filters_text}
        4. Extrae informaci√≥n de las primeras {max_results} ofertas:
           - T√≠tulo del puesto (title)
           - Empresa (company)
           - Ubicaci√≥n (location)
           - Nivel (level) si est√° visible (ej: "Mid-Senior", "Entry level")
           - Tipo de empleo (employment_type) si visible (ej: "Full-time", "Contract")
           - URL directa a la oferta (url)
           - Descripci√≥n corta (description) - primeras 2-3 l√≠neas
        5. Organiza los datos en una lista de diccionarios JSON con las claves indicadas
        6. Convierte la lista a un JSON string
        7. Llama a guardar_ofertas pasando el JSON string como par√°metro jobs_json

        IMPORTANTE:
        - Si LinkedIn pide login, intenta acceder a trabajos p√∫blicos sin iniciar sesi√≥n
        - Si aparecen modales, ci√©rralos
        - Extrae solo ofertas reales
        - Si no encuentras alg√∫n campo (como level), d√©jalo como null
        - El par√°metro jobs_json debe ser un string JSON, no un objeto
        """

        agent = Agent(
            task=task,
            llm=llm,
            browser=browser,
            use_vision=True,
            tools=self.tools,
            save_conversation_path="logs/linkedin_conversation.json"
        )

        try:
            print(f"üöÄ Iniciando scraping de LinkedIn: '{search_query}' en {location}")
            await agent.run()
            print(f"‚úÖ LinkedIn scraping completado! {len(self.scraped_jobs)} ofertas")
            return self.scraped_jobs
        except Exception as error:
            print(f"‚ùå Error durante scraping de LinkedIn: {error}")
            return []


async def scrape_linkedin_jobs(
    search_query: str,
    location: str = "Spain",
    max_results: int = 10,
    remote_only: bool = True
):
    """Funci√≥n helper para ejecutar el scraper"""
    scraper = LinkedInScraper()
    return await scraper.scrape(
        search_query=search_query,
        location=location,
        max_results=max_results,
        remote_only=remote_only
    )


if __name__ == "__main__":
    # Test del scraper
    asyncio.run(scrape_linkedin_jobs(
        search_query="Python developer",
        location="Spain",
        max_results=10
    ))
